
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Name Surname">
    <link rel="icon" href="media/favicon.ico">

    <title>Fab Academy 2015 - Sabina Barcucci</title>
      <link href='http://fonts.googleapis.com/css?family=Karla:400,400italic,700,700italic' rel='stylesheet' type='text/css'>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="media/fabacademy.css" rel="stylesheet">       <link href="media/SABINA.css" rel="stylesheet">
    
    <!-- 3D files viewer -->
    <script type="text/javascript" src="media/jsc3d_ie.min.js"></script>
	<script type="text/javascript" src="media/jsc3d.min.js"></script>
    <script type="text/javascript" src="media/jsc3d.webgl.js"></script>
    <script type="text/javascript" src="media/jsc3d.touch.js"></script>

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="bootstrap/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="bootstrap/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <!-- Load the menu file -->
    <script>
	function menu() {
					  $('#exercises').load("exercises-menu.html");
					  $('#project').load("project-menu.html");
					  $('#cclicense').load("license.html");
					  }
	</script>

  </head>

  <body onload="menu()">

    <!-- Static navbar -->
    <nav class="navbar navbar-default navbar-static-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Sabina Barcucci</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="about.html">About</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Modules <span class="caret"></span></a>
              <ul id="exercises" class="dropdown-menu" role="menu">
              </ul>
            </li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Breathing Pod <span class="caret"></span></a>
              <ul id="project" class="dropdown-menu" role="menu">
              </ul>
            </li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

	<!-- Insert your content here below! -->
        
    <h4><em>‘I am part of the networks and the networks are part of me...
I link, therefore I am.’</em>       <br>William. J. Mitchell</h4>
        <h1><strong>Neuro Toy </strong></h1>  
        <br>
        <br>
    
        <img src="img/pritdwmxmbdl9cggbkpx-02.jpg">
        <br>
        <br>
        <h3><strong>Technological frame</strong></h3>
       
   
        <p>
        Brain-computer interfaces (BCIs) have been around since the 1970s, when clunky EEG readers were used in laboratory settings for rudimentary neurofeedback and biofeedback programs. Although the readings and data inputs from EEG readers have not changed significantly over the past forty-odd years, the equipment used has changed significantly. Instead of requiring a university laboratory or a quiet room without sounds from the outside causing false positives, and instead of requiring nurses or lab technicians to assist with setup, they have become consumer technology.</p>
        <br>
      
        Today's brain-reading headbands require no medical training to use, have a tiny learning curve, and are suitable as entertaining objects.

The main concept in brain-machine interfaces is that changes in human brain are reflected in changes in some signal, as EEG, which can then be used as a kind of control action to a machine, without the need of using any physical action or command. Our brainwaves (EEG) are small electrical potentials on our scalp as a result of neurons firing inside brain. Headband electrodes record this fluctuating voltage several hundred times per second. These voltages are converted to digital signals and a stream of numbers is sent to a PC or Mac via Bluetooth. <br> <a href="http://www.fastcompany.com/3008499/tech-forecast/these-brain-scanning-neuro-toys-are-about-change-everythingr">[source] </a>
        </p>
      
    <br>
        <br>
      <img src="img/eeg_kiss-01.png">
      <br> <br>
        <h3><strong>Idea and References</strong></h3>
    
        <p>

I want to create a wearable prototype of a neuro toy focusing on augmented sensing and/or augmented human-to-human communication, providing some feedbacks to the user on her emotions. The project should be developed in the perspective of a museum experimental “BYOB” (Bring Your Own Brain or Body) installation in which visitors could testing and learning about their feelings and physiological reactions. 
            <p>
            
The general framework could be then related to neurofeedback measurements of physiological activity such as brainwaves (using EEG) during a specific activity and creating a set of feedbacks perceivable or only by the user or instead collectively perceivable (as visual or sound feedback). <a href="http://www.lancelmaat.nl/work/e.e.g-kiss/">EEG Kiss</a> gives an oustanding example of a potential use of an EEG headset connected to a visual feedback device for an artistic installation. Focusing on neurofeedback means to target the wearable device on and headband and/or dataglove-like device (in case it would appear necessary adding other physiological measurements such as heart function, breathing, muscle activity, and skin temperature via EDG or ECG). I want to involve especially the ears as regions interested by the wearable device since our high sensibility in controlling and managing this area. <br>
                <p>
            

            <img src="img/muse_headband.jpg"> 
                    A potential start could be looking into the <a href="http://www.choosemuse.com/">Muse Headband</a>, which helps monitoring physiological activities on the brain and improve concentration, focus, and relaxation. I am not aiming to control anything with brainwaves, as MUSE does, but instead to detect specific user emotions and playfully work with them. <br><br><br>
            
           
            
        
 </p>
      <h2><strong>First sketches</strong></h2><br>
      <p>
      A very first idea on how to apply these neurofeedback wearable technologies could be to enhance the user emotions, making them visible to the surrounding environment (e.g. through a light feedback). Users can set the headband in order to make it displaying more lightfully according to the personal user's mood. EEG detection will be able to translate a specific mood into a light sequence, making the user feelings more sharable with people aroung. I can see this enhancer as a good toy for pleasant contexts such as parties, concerts, relaxing times (such as sauna time) or while making exciting activities as snowboarding, etc. This device idea get more sense if imagined be used in public space and with other users wearing the same device. </p>
        <br>
      
<img src="img/headband-02.jpg"><br><br>
<img src="img/headband-03.jpg"><br><br>
      <p>
      An interesting element that such headband with such properties would bring in the design phase, is the study of the ergonomy of the tool in relation with the possibility of using part of the user face as buttons to control th headband. Instead of touching a device element to on/off it, it sould be interesting to test the potentiality of the ears surface to communicate with the device itself. Below a very preliminary sketch of a possible use of the ear surface to control headband modes, if more than one.  </p>
    <img src="img/first_sketch_Neuro_Toy-01.jpg"><br><br>
        
      
            </p>
   <img src="img/byob-01.jpg"><br><br>
        <h3><strong>BYOB/B: Bring Your Own Brain/Body</strong></h3><br>
      <p>
       Usually, we use our ears to listen. In the terminology of human–computer interaction, this means that the ears are used to consume output from the computer.

But the ear’s surface can also be used for input to communicate commands from the user to the computer.

Roman Lissermann, Jochen Huber, Aristotelis Hadjakos, and Max Mühlhäuser from the Technical University of Darmstadt (also in Germany) presented a research prototype called "EarPut" to do just that. Among other benefits, your ear is always in the same place; touching your ear is also slightly less obtrusive than touching your hand.

Possible interactions include:

    Touching part of the ear surface, with either single- or multi-touch.
    Tugging an earlobe. This interaction is particularly suited for on–off commands, such as muting a music player.
    Sliding a finger up or down along the ear arc. This might work well for adjusting volume up or down.
    Covering the ear — certainly a natural gesture for "mute."

To measure how precisely people can touch their own ears in a simple single-touch interaction, Lissermann and colleagues instrumented 27 users' ears. When they divided the ear arc into only 2 regions, participants achieved 99% accuracy. When a 3rd region was introduced, however, accuracy dropped substantially.

As this research shows, ear-driven input is best for situations with an extremely limited number of commands like On / Off. 
          <a href="http://www.nngroup.com/articles/human-body-touch-input/">[source] </a>
<br> <br>

The image above shows the <a href="http://asg.sutd.edu.sg/project/earput">EarPut Prototype </a> hardware: it’s easy to imagine smaller, lighter, and more elegant hardware.</p>
            <p>
            
            
        
            
            <br>
            
            
            
            <br>
            <img src="img/EarPut-600x422.jpg">
            
            </p>
            
<!--	<p>
	You can edit it by changing the <em>exercises-menu.html</em> and <em>project-menu.html</em>. Do <strong>not</strong> add any &ltol&gt; or &ltul&gt;, leave the structure as it is.
	</p>
-->
    <!--
    
    <h3>Text</h3>
	<p>
	You can set it up with:
	<pre class="prettyprint linenums">
	&ltp&gt;
	Your text here
	&lt/p&gt;</pre>
	</p>
	Result:
	<p>
	Your text here
	</p>
	
	<h3>Images</h3>
	<p>
	You can set it up with:
	<pre class="prettyprint linenums">
	&ltp class="pic&gt;>&ltimg src="http://academy.cba.mit.edu/2012/students/menichinelli.massimo/exercise07/exercise07_02.jpg"&gt;
	&ltlegend&gt;Here's the legend of the picture.&lt/legend&gt;
	&lt/p&gt;</pre>
	</p>
	Result:
	<p class="pic"><img src="http://academy.cba.mit.edu/2012/students/menichinelli.massimo/exercise07/exercise07_02.jpg">
	<legend>Here's the legend of the picture.</legend>
	</p>
     
	<h3>Code</h3>
	<p>
	You can set it up with:
	<pre class="prettyprint linenums">
	&ltpre class="prettyprint linenums"&gt;
	Your code here
	&lt/pre&gt;</pre>
	
	Result:
	<pre class="prettyprint linenums">
	for i in members:
		print ""
		print "USER:",members[i]
		print "Loading connections..."
		followers = load_connections([i], "followers")
		friends = load_connections([i], "friends")

		# Add edges...
		print "Building the graph..."

		for f in followers:
			for k in followers[f]:
				graph.add_edge(k,f)
	
		for f in friends:
			for k in friends[f]:
				graph.add_edge(f,k)</pre>
	</p>
	
	<h3>3D Model</h3>
	<p>	
	The model can be rotated and zoomed with the mouse/trackpad. You can set it up with:
	<pre class="prettyprint linenums">
	&ltcanvas id="cv" width=640 height=480&gt;
	It seems you are using an outdated browser that does not support canvas :-(
	&lt/canvas&gt;
	&ltscript type="text/javascript"&gt;
		var canvas = document.getElementById('cv');
		var viewer = new JSC3D.Viewer(canvas);
		viewer.setParameter('SceneUrl','media/monkey.stl');
		viewer.setParameter('ModelColor','#CAA618');
		viewer.setParameter('BackgroundColor1','#E5D7BA');
		viewer.setParameter('BackgroundColor2','#383840');
		viewer.setParameter('RenderMode','flat');
		viewer.setParameter('MipMapping','on');
		viewer.setParameter('Definition','high');
		viewer.setParameter('Renderer','webgl');
		viewer.init();
		viewer.update();
	&lt/script&gt;</pre>
	
	You can set up the path to the 3D model (.stl/.obj) within this line:
	<pre class="prettyprint linenums">
	viewer.setParameter('SceneUrl','media/monkey.stl');</pre>
	
	Result:
	<canvas id="cv" width=640 height=480>
	It seems you are using an outdated browser that does not support canvas :-(
	</canvas>
	<script type="text/javascript">
		var canvas = document.getElementById('cv');
		var viewer = new JSC3D.Viewer(canvas);
		viewer.setParameter('SceneUrl','media/monkey.stl');
		viewer.setParameter('ModelColor','#CAA618');
		viewer.setParameter('BackgroundColor1','#E5D7BA');
		viewer.setParameter('BackgroundColor2','#383840');
		viewer.setParameter('RenderMode','flat');
		viewer.setParameter('MipMapping','on');
		viewer.setParameter('Definition','high');
		viewer.setParameter('Renderer','webgl');
		viewer.init();
		viewer.update();
	</script> 
    </p> 
    
    <h3>Download</h3>
	<p>
	You can set up a button for files to be downloaded with:
	<pre class="prettyprint linenums">
	&lta href="#"&gt;
	&ltbutton type="button" class="btn btn-primary btn-lg">Download the file&lt/button&gt;
	&lt/a&gt;</pre>
	</p>
	
	<p>
	Result:
	</p>
	<p>
	<a href="#">
	<button type="button" class="btn btn-primary btn-lg">Download the file</button>
	</a>
	</p>
      
    <h3>Would you like to insert more style/elements?</h3>
      <p>
      Check the documentation of Bootstrap <a href="">here</a>.
      </p>
-->
    <!-- End of your content -->

    </div> <!-- /container -->

	<!-- footer -->
    
    <footer id="footer">
        <p id="cclicense">
                </p>
        <p class="license">
        Theme: <a href="https://github.com/openp2pdesign/FabAcademy_Template">Fab Academy Template</a> by <a href="http://openp2pdesign.org">Massimo Menichinelli</a> <br>
        Based on <a href="http://getbootstrap.com/">Twitter Bootstrap</a>+<a href="http://jquery.com/">JQuery</a>+<a href="https://code.google.com/p/google-code-prettify/">google-code-prettify</a>+<a href="http://jmblog.github.io/color-themes-for-google-code-prettify/github/">GitHub theme for google-code-prettify</a>+<a href="https://code.google.com/p/jsc3d/">JSC3D</a>+<a href="https://github.com/thegrubbsian/jquery.ganttView">jquery.ganttView</a>.
        </p>
    </footer>
    

	<!-- Do not touch this! -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="media/jquery-1.9.1.min.js"></script>
    
    <!-- Syntax Highlighter -->
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js">
    </script>
    <!-- From https://github.com/jmblog/color-themes-for-google-code-prettify -->
    <link href="media/github.css" type="text/css" rel="stylesheet">
	<script type="text/javascript">
	  !function ($) {
		$(function(){
		  window.prettyPrint && prettyPrint()   
		})
	  }(window.jQuery)
	</script>
	
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="bootstrap/js/ie10-viewport-bug-workaround.js"></script>

  </body>
</html>
